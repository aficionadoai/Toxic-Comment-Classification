{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NB-SVM with TFIDF + SMBO.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markerenberg/Toxic-Comment-Classification/blob/master/NB_SVM_with_TFIDF_%2B_SMBO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6AvEhk7_upV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check if GPU is enabled\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHXV-Uu90Ldo",
        "colab_type": "code",
        "outputId": "62401d6b-203b-4dc6-bc05-2cf83c8d90ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "##\n",
        "## =======================================================\n",
        "## Mark Erenberg \n",
        "## Toxic Comment Classification Challenge\n",
        "## =======================================================\n",
        "##\n",
        "\n",
        "# Objective: Create a model which predicts a probability of each type of toxicity for each comment.\n",
        "\n",
        "# import dependencies and files\n",
        "\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "from scipy.sparse import hstack\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import tempfile\n",
        "import warnings\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "import nltk\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag, word_tokenize\n",
        "\n",
        "import gensim\n",
        "import gensim.models.keyedvectors as word2vec\n",
        "from gensim.models.fasttext import FastText\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "import spacy\n",
        "import en_core_web_sm\n",
        "spacy_nlp = en_core_web_sm.load()\n",
        "spacy_nlp = spacy.load('en_core_web_sm')\n",
        "from spacy.lemmatizer import Lemmatizer\n",
        "\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "from sklearn import utils\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, average_precision_score\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Model\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import Callback\n",
        "\n",
        "################### Data Loading ###################\n",
        "#os.chdir('C:\\\\Users\\\\marke\\\\Downloads\\\\Toxic Classification')\n",
        "train = pd.read_csv('train.csv').fillna('')\n",
        "test = pd.read_csv('test.csv').fillna('')\n",
        "\n",
        "train_text = train[['id','comment_text']].drop_duplicates()\n",
        "df = pd.concat([train_text,test],axis=0,ignore_index=True)\n",
        "\n",
        "################### Data Cleaning ####################\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "\n",
        "wpt = nltk.WordPunctTokenizer()\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "cv = CountVectorizer(min_df=0., max_df=1.)\n",
        "\n",
        "# Simple way to get the number of occurence of a regex\n",
        "def count_regexp_occ(regexp=\"\", text=None):\n",
        "    return len(re.findall(regexp, text))\n",
        "\n",
        "# Determine if file words exist:\n",
        "#print(len(df[df['comment_text'].str.contains('jpg')]))\n",
        "#print(len(df[df['comment_text'].str.contains('jpeg')]))\n",
        "#print(len(df[df['comment_text'].str.contains('http')]))\n",
        "#print(len(df[df['comment_text'].str.contains('pdf')]))\n",
        "#print(len(df[df['comment_text'].str.contains('html')]))\n",
        "\n",
        "# Remove non-alphabetic characters and split tokens by spaces/newlines\n",
        "def clean_document(doc):\n",
        "    # 1) Convert string to lower\n",
        "    #doc = bytes(doc.lower(), encoding=\"utf-8\")\n",
        "    doc = doc.lower()\n",
        "    # 2) Replace contracion patterns\n",
        "    cont_patterns = [\n",
        "    (r'(W|w)on\\'t', r'will not'),\n",
        "    (r'(C|c)an\\'t', r'can not'),\n",
        "    (r'(I|i)\\'m', r'i am'),\n",
        "    (r'(A|a)in\\'t', r'is not'),\n",
        "    (r'(\\w+)\\'ll', r'\\g<1> will'),\n",
        "    (r'(\\w+)n\\'t', r'\\g<1> not'),\n",
        "    (r'(\\w+)\\'ve', r'\\g<1> have'),\n",
        "    (r'(\\w+)\\'s', r'\\g<1> is'),\n",
        "    (r'(\\w+)\\'re', r'\\g<1> are'),\n",
        "    (r'(\\w+)\\'d', r'\\g<1> would'),\n",
        "    ]\n",
        "    patterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]\n",
        "    for (pattern, repl) in patterns:\n",
        "        doc = re.sub(pattern, repl, doc)\n",
        "    # 3) Remove special characters\\whitespaces\n",
        "    doc = re.sub(r'[^a-zA-Z\\s]+', '', doc)\n",
        "    #doc = doc.encode('utf-8')\n",
        "    #doc = str(doc,'utf-8').strip()\n",
        "    doc = doc.strip()\n",
        "    # tokenize document\n",
        "    tokens = wpt.tokenize(doc)\n",
        "    # filter stopwords out of document\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    # re-create document from filtered tokens\n",
        "    doc = ' '.join(filtered_tokens)\n",
        "    #doc = ' '.join(tokens)\n",
        "    return doc\n",
        "\n",
        "# Lemmaitze comments:\n",
        "def lemmatize_comment(comment):\n",
        "        doc = spacy_nlp(comment)\n",
        "        return [token.lemma_ for token in doc if token.lemma_ != '-PRON-' ]         \n",
        "                \n",
        "#df['clean_comments'] = [clean_document(x) for x in df['comment_text']]\n",
        "#df['clean_comments_list'] = df['clean_comments'].apply(lambda x: x.split())\n",
        "#df['clean_lemmed'] = [lemmatize_comment(x) for x in df['clean_comments']]\n",
        "#df['clean_lemmed_str'] = df['clean_lemmed'].apply(lambda x: \" \".join(x))\n",
        "train['clean_comments'] = [clean_document(x) for x in train['comment_text']]\n",
        "train['clean_comments_list'] = train['clean_comments'].apply(lambda x: x.split())\n",
        "train['clean_lemmed'] = [lemmatize_comment(x) for x in train['clean_comments']]\n",
        "train['clean_lemmed_str'] = train['clean_lemmed'].apply(lambda x: \" \".join(x))\n",
        "\n",
        "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HsGQpdB9hFMJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "4909953d-1007-4422-d86b-5df8989505a5"
      },
      "source": [
        "################### Compare with baseline TF-IDF features ###################\n",
        "\n",
        "# TF-IDF Vectorizer\n",
        "train_text = train['clean_lemmed_str']\n",
        "\n",
        "word_vectorizer = TfidfVectorizer(\n",
        "    min_df = 3,\n",
        "    max_df = 0.9,\n",
        "    sublinear_tf=True,\n",
        "    smooth_idf=True,\n",
        "    strip_accents='unicode',\n",
        "    analyzer='word',\n",
        "    token_pattern=r'\\w{1,}',\n",
        "    stop_words='english',\n",
        "    ngram_range=(1, 2),\n",
        "    max_features=20000)\n",
        "word_vectorizer.fit(train_text)\n",
        "train_word_features = word_vectorizer.transform(train_text)\n",
        "\n",
        "char_vectorizer = TfidfVectorizer(\n",
        "    min_df = 3,\n",
        "    max_df = 0.9,\n",
        "    sublinear_tf=True,\n",
        "    strip_accents='unicode',\n",
        "    analyzer='char',\n",
        "    stop_words='english',\n",
        "    ngram_range=(2, 6),\n",
        "    max_features=20000)\n",
        "char_vectorizer.fit(train_text)\n",
        "train_char_features = char_vectorizer.transform(train_text)\n",
        "\n",
        "#train_tfidf_features = hstack([train_char_features, train_word_features]).tocsr()\n",
        "train_tfidf_features = train_word_features.tocsr()\n",
        "\n",
        "# Create features about type of text and category of text\n",
        "def add_features(df):\n",
        "    # Get length in words and characters\n",
        "    df[\"word_count\"] = df[\"comment_text\"].apply(lambda x: len(x.split()))\n",
        "    df[\"word_len_avg\"] = df[\"comment_text\"].apply(lambda x: np.mean([len(x) for x in x.split()]))\n",
        "    df[\"word_len_std\"] = df[\"comment_text\"].apply(lambda x: np.std([len(x) for x in x.split()]))\n",
        "    df[\"char_count\"] = df[\"comment_text\"].apply(lambda x: len(x))\n",
        "    # Create count variables to see if any are useful\n",
        "    df[\"upper_ratio\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[A-Z]\", x)) /df['char_count']*100\n",
        "    df[\"number_ratio\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[0-9]\", x)) / df[\"char_count\"] *100\n",
        "    df[\"excl_ratio\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"!\", x)) / df[\"char_count\"] *100\n",
        "    df[\"quest_ratio\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\?\", x)) / df[\"char_count\"] *100\n",
        "    df[\"equals_ratio\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"=\", x)) / df[\"char_count\"] *100\n",
        "    df[\"punct_ratio\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[.!\\?=+#*|~-]\", x)) / df[\"char_count\"] *100\n",
        "    df[\"you_ratio\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\W[Yy]ou\\W\", x)) / df[\"word_count\"] *100\n",
        "    df[\"nb_fuck\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[Ff][uU][cC][Kk]\", x))\n",
        "    df[\"nb_suck\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[Ss]\\S{2}[Kk]\", x))\n",
        "    df[\"nb_dick\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[dD]ick\", x))\n",
        "    df[\"nb_penis\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[pP][eE][nN][iI][sS]\", x))\n",
        "    df[\"nb_pussy\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[pP][uU][sS][sS][yY]\", x))\n",
        "    df[\"nb_cock\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[cC]ock\", x))\n",
        "    df[\"8==D\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"8=+D\", x))\n",
        "    df[\"nb_gay\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[gG][aA][yY]\", x))\n",
        "    df[\"nb_bitch\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[bB][iI][tT][cC][hH]\", x))\n",
        "    df[\"nb_cunt\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[cC][uU][nN][tT]\", x))\n",
        "    df[\"nb_shut_up\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[sS][hH][uU][tT]\\s[uU][pP]\", x))\n",
        "    df[\"nb_mother\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wmother\\W\", x))\n",
        "    df[\"nb_ng\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wnigger\\W\", x))\n",
        "    df[\"nb_ng_2\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wnigga\\W\", x))\n",
        "\n",
        "def pos_tagging(df):\n",
        "    df[\"comment_raw\"] = df[\"comment_text\"].apply(lambda x: re.sub(r'[\\n\\r\\t\\r\\n]',r' ',x,re.I|re.U))\n",
        "    df[\"POS\"] = df[\"comment_raw\"].apply(lambda x: [token.pos_ for token in spacy_nlp(x)])\n",
        "    \n",
        "def pos_features(df):\n",
        "    # Get number of proper nouns\n",
        "    df[\"PROPN\"] = df[\"POS\"].apply(lambda x: len([pos for pos in x if pos == 'PROPN']))\n",
        "    df[\"ADJ\"] = df[\"POS\"].apply(lambda x: len([pos for pos in x if pos == 'ADJ']))\n",
        "    df[\"INTJ\"] = df[\"POS\"].apply(lambda x: len([pos for pos in x if pos == 'INTJ']))\n",
        "    df[\"SYM\"] = df[\"POS\"].apply(lambda x: len([pos for pos in x if pos == 'SYM']))\n",
        "\n",
        "    \n",
        "#add_features(train)\n",
        "#pos_tagging(df)\n",
        "#pos_features(df)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:520: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\"The parameter 'stop_words' will not be used\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiYfLT5uX6RG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy import sparse\n",
        "class NbSvmClassifier(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, C=1.0, tol=1e-4, dual=False, n_jobs=1, multi_class='auto'):\n",
        "        self.C = C\n",
        "        self.tol=tol\n",
        "        self.dual = dual\n",
        "        self.n_jobs = n_jobs\n",
        "        self.multi_class = multi_class\n",
        "\n",
        "    def predict(self, x):\n",
        "        # Verify that model has been fit\n",
        "        check_is_fitted(self, ['_r', '_clf'])\n",
        "        return self._clf.predict(x.multiply(self._r))\n",
        "\n",
        "    def predict_proba(self, x):\n",
        "        # Verify that model has been fit\n",
        "        check_is_fitted(self, ['_r', '_clf'])\n",
        "        return self._clf.predict_proba(x.multiply(self._r))\n",
        "\n",
        "    def fit(self, x, y):\n",
        "        # Check that X and y have correct shape\n",
        "        y = y.values\n",
        "        x, y = check_X_y(x, y, accept_sparse=True)\n",
        "\n",
        "        def pr(x, y_i, y):\n",
        "            p = x[y==y_i].sum(0)\n",
        "            return (p+1) / ((y==y_i).sum()+1)\n",
        "\n",
        "        self._r = sparse.csr_matrix(np.log(pr(x,1,y) / pr(x,0,y)))\n",
        "        x_nb = x.multiply(self._r)\n",
        "        self._clf = LogisticRegression(C=self.C, tol=self.tol, dual=self.dual, n_jobs=self.n_jobs).fit(x_nb, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3tTu71X3gUp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### Fit NBSVM Model #####\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "train_features = train_tfidf_features\n",
        "\n",
        "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "seed = 1234\n",
        "splits = 5\n",
        "folds = StratifiedKFold(n_splits=splits, shuffle=True, random_state=seed)\n",
        "\n",
        "for i, j in enumerate(class_names):\n",
        "    training_labels = train[j]\n",
        "    class_pred = np.zeros(len(train))\n",
        "    auc,precision,recall,thresholds = [],[],[],[]\n",
        "\n",
        "    # Make predictions for each fold in split, calculate evaluation metrics\n",
        "    for n_fold, (trn_idx, val_idx) in enumerate(folds.split(train_features, training_labels)):\n",
        "      # Train NB-SVM\n",
        "      model = NbSvmClassifier(C=4, tol=1e-4, dual=False, n_jobs=-1,multi_class='auto').fit(train_features[trn_idx], training_labels[trn_idx])\n",
        "      class_pred[val_idx] = model.predict_proba(train_features[val_idx])[:,1]\n",
        "      auc.append(roc_auc_score(training_labels[val_idx], class_pred[val_idx],average='weighted'))\n",
        "      prec, recal, thresh = precision_recall_curve(training_labels[val_idx], class_pred[val_idx])\n",
        "      precision.append(prec)\n",
        "      recall.append(recal)\n",
        "      thresholds.append(thresh)\n",
        "\n",
        "    # Print out mean AUC score\n",
        "    print(\"fit: \",j,' | mean AUC: ',str(round(np.mean(auc),4)))\n",
        "    print(\"Thresholds: \"+str(thresholds[4]))\n",
        "    print(\"Precision: \"+str(precision[4]))\n",
        "    # Plot precision-recall curve\n",
        "    plt.figure()\n",
        "    plt.plot(recall[0], precision[0], 'ro', linewidth=2,label='Fold 1 Preds')\n",
        "    plt.plot(recall[1], precision[1], 'bo', linewidth=2,label='Fold 2 Preds')\n",
        "    plt.plot(recall[2], precision[2], 'go', linewidth=2,label='Fold 3 Preds')\n",
        "    plt.plot(recall[3], precision[3], 'yo', linewidth=2,label='Fold 4 Preds')\n",
        "    plt.plot(recall[4], precision[4], 'mo', linewidth=2,label='Fold 5 Preds')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Class: '+j+' | Average Precision: '+str(average_precision_score(training_labels,class_pred,average='weighted')))\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgseRNtSa4O8",
        "colab_type": "code",
        "outputId": "0a1aa1c2-a21b-49cb-b53a-27d932f2c191",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "##### Bayesian Hyperparameter Tuning for Severe Toxic Class #####\n",
        "\n",
        "#warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "train_features = train_tfidf_features\n",
        "\n",
        "class_names_2 = ['severe_toxic', 'threat', 'identity_hate']\n",
        "seed = 1234\n",
        "splits = 5\n",
        "folds = StratifiedKFold(n_splits=splits, shuffle=True, random_state=seed)\n",
        "\n",
        "j = 'severe_toxic'\n",
        "training_labels = train[j]\n",
        "class_pred = np.zeros(train_features.shape[0])\n",
        "auc,precision,recall,thresholds = [],[],[],[]\n",
        "\n",
        "### Define functions for Bayesian Hyperparameter Optimization (SMBO using TPE)\n",
        "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
        "from sklearn.metrics import r2_score, roc_auc_score, accuracy_score, f1_score, confusion_matrix, classification_report, roc_curve, make_scorer\n",
        "\n",
        "# 1) Define objective \n",
        "\n",
        "def objective(params):\n",
        "    time1 = time.time()\n",
        "    params = {\n",
        "        'C': params['C'],\n",
        "        'tol': params['tol'],\n",
        "    }\n",
        "\n",
        "    print(\"\\n############## New Run ################\")\n",
        "    print(f\"params = {params}\")\n",
        "    seed = 1234\n",
        "    splits = 5\n",
        "    folds = StratifiedKFold(n_splits=splits, shuffle=True, random_state=seed)\n",
        "    score_mean = 0\n",
        "    class_pred = np.zeros(train_features.shape[0])\n",
        "\n",
        "    for n_fold, (trn_idx, val_idx) in enumerate(folds.split(train_features, training_labels)):\n",
        "      clf = NbSvmClassifier(C=params['C'], tol=params['tol'], dual=False,n_jobs=-1,multi_class='auto')\n",
        "      clf.fit(train_features[trn_idx], training_labels[trn_idx])\n",
        "      class_pred[val_idx] = clf.predict_proba(train_features[val_idx])[:,1]\n",
        "      score = average_precision_score(training_labels[val_idx],class_pred[val_idx],average='weighted')\n",
        "      score_mean += score\n",
        "    time2 = time.time() - time1\n",
        "    print(f\"Total Time Run: {round(time2 / 60,2)}\")\n",
        "    #gc.collect()\n",
        "    print(f'Average Precision Score: {score_mean/splits}')\n",
        "    return -(score_mean / splits)\n",
        "\n",
        "\n",
        "# 2) Define search space\n",
        "\n",
        "space = {\n",
        "    'C': hp.lognormal('C', np.log(1e-6),np.log(1)),\n",
        "    'tol': hp.choice('tol',[1e-7,1e-6,1e-5,1e-4,1e-3,1e-2])\n",
        "}    \n",
        "\n",
        "# 3) Specify Optimization algorithm\n",
        "tpe_algo = tpe.suggest\n",
        "\n",
        "# 4) Instantiate Trials object to track results\n",
        "tpe_trials = Trials()\n",
        "\n",
        "# Set hyperopt parameters\n",
        "best = fmin(fn=objective,\n",
        "            space=space,\n",
        "            algo=tpe_algo,\n",
        "            trials = tpe_trials,\n",
        "            max_evals=1000)\n",
        "# Print best parameters\n",
        "best_params = space_eval(space, best)\n",
        "print(\"BEST PARAMS: \", best_params)\n",
        "\n",
        "# Make predictions for each fold in split, calculate evaluation metrics\n",
        "for n_fold, (trn_idx, val_idx) in enumerate(folds.split(train_features, training_labels)):\n",
        "  # Train NB-SVM\n",
        "  model = NbSvmClassifier(C=best_params['C'], tol=best_params['tol'], dual=False, n_jobs=-1,multi_class='auto')\n",
        "  model.fit(train_features[trn_idx], training_labels[trn_idx])\n",
        "  class_pred[val_idx] = model.predict_proba(train_features[val_idx])[:,1]\n",
        "  auc.append(roc_auc_score(training_labels[val_idx], class_pred[val_idx],average='weighted'))\n",
        "  prec, recal, thresh = precision_recall_curve(training_labels[val_idx], class_pred[val_idx])\n",
        "  precision.append(prec)\n",
        "  recall.append(recal)\n",
        "  thresholds.append(thresh)\n",
        "\n",
        "# Print out mean AUC score\n",
        "print(\"fit: \",j,' | mean AUC: ',str(round(np.mean(auc),4)))\n",
        "# Plot precision-recall curve\n",
        "plt.figure()\n",
        "plt.plot(recall[0], precision[0], 'ro', linewidth=2,label='Fold 1 Preds')\n",
        "plt.plot(recall[1], precision[1], 'bo', linewidth=2,label='Fold 2 Preds')\n",
        "plt.plot(recall[2], precision[2], 'go', linewidth=2,label='Fold 3 Preds')\n",
        "plt.plot(recall[3], precision[3], 'yo', linewidth=2,label='Fold 4 Preds')\n",
        "plt.plot(recall[4], precision[4], 'mo', linewidth=2,label='Fold 5 Preds')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Class: '+j+' | Average Precision: '+str(average_precision_score(training_labels,class_pred,average='weighted')))\n",
        "plt.legend()\n",
        "plt.show()\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "############## New Run ################\n",
            "params = {'C': 1.0000000000000004e-06, 'tol': 0.0001}\n",
            "Total Time Run: 0.06\n",
            "Average Precision Score: 0.3167251689751083\n",
            "\n",
            "############## New Run ################\n",
            "params = {'C': 1.0000000000000004e-06, 'tol': 1e-05}\n",
            "Total Time Run: 0.06\n",
            "Average Precision Score: 0.3167251689751083\n",
            "\n",
            "############## New Run ################\n",
            "params = {'C': 1.0000000000000004e-06, 'tol': 1e-05}\n",
            "  0%|          | 2/1000 [00:06<56:49,  3.42s/it, best loss: -0.3167251689751083]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o2tN6hHLunu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features = train_tfidf_features\n",
        "\n",
        "class_names_2 = ['severe_toxic', 'threat', 'identity_hate']\n",
        "seed = 1234\n",
        "splits = 5\n",
        "folds = StratifiedKFold(n_splits=splits, shuffle=True, random_state=seed)\n",
        "\n",
        "j = 'severe_toxic'\n",
        "training_labels = train[j]\n",
        "class_pred = np.zeros(train_features.shape[0])\n",
        "auc,precision,recall,thresholds = [],[],[],[]\n",
        "\n",
        "### Define functions for Bayesian Hyperparameter Optimization (SMBO using TPE)\n",
        "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
        "from sklearn.metrics import r2_score, roc_auc_score, accuracy_score, f1_score, confusion_matrix, classification_report, roc_curve, make_scorer\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8o8yLUeL-Ab",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "6c47d36e-33b3-44d1-8a61-cda776d732df"
      },
      "source": [
        "# 1) Define objective \n",
        "\n",
        "def objective(params):\n",
        "    time1 = time.time()\n",
        "    params = {\n",
        "        'C': params['C'],\n",
        "        'tol': params['tol'],\n",
        "    }\n",
        "\n",
        "    print(\"\\n############## New Run ################\")\n",
        "    print(f\"params = {params}\")\n",
        "    seed = 1234\n",
        "    splits = 5\n",
        "    folds = StratifiedKFold(n_splits=splits, shuffle=True, random_state=seed)\n",
        "    score_mean = 0\n",
        "    class_pred = np.zeros(train_features.shape[0])\n",
        "\n",
        "    for n_fold, (trn_idx, val_idx) in enumerate(folds.split(train_features, training_labels)):\n",
        "      clf = NbSvmClassifier(C=params['C'], tol=params['tol'], dual=False,n_jobs=-1,multi_class='auto')\n",
        "      clf.fit(train_features[trn_idx], training_labels[trn_idx])\n",
        "      #print(str(train_features[val_idx].shape)+\" and \"+str(training_labels[val_idx].shape))\n",
        "      class_pred[val_idx] = clf.predict_proba(train_features[val_idx])[:,1]\n",
        "      #print(str(train_features[val_idx].shape)+\" and \"+str(training_labels[val_idx].shape)+\" and \"+str(class_pred[val_idx].shape))\n",
        "      score = average_precision_score(training_labels[val_idx],class_pred[val_idx],average='weighted')\n",
        "      score_mean += score\n",
        "    time2 = time.time() - time1\n",
        "    print(f\"Total Time Run: {round(time2 / 60,2)}\")\n",
        "    print(f'Average Precision Score: {score_mean/splits}')\n",
        "    return -(score_mean / splits)\n",
        "\n",
        "\n",
        "# 2) Define search space\n",
        "\n",
        "space = {\n",
        "    'C': hp.lognormal('C', np.log(1e-6),np.log(1)),\n",
        "    'tol': hp.choice('tol',[1e-7,1e-6,1e-5,1e-4,1e-3,1e-2])\n",
        "}    \n",
        "\n",
        "# 3) Specify Optimization algorithm\n",
        "tpe_algo = tpe.suggest\n",
        "\n",
        "# 4) Instantiate Trials object to track results\n",
        "tpe_trials = Trials()\n",
        "\n",
        "# Set hyperopt parameters\n",
        "best = fmin(fn=objective,\n",
        "            space=space,\n",
        "            algo=tpe_algo,\n",
        "            trials = tpe_trials,\n",
        "            max_evals=1000)\n",
        "# Print best parameters\n",
        "best_params = space_eval(space, best)\n",
        "print(\"BEST PARAMS: \", best_params)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "############## New Run ################\n",
            "params = {'C': 1.0000000000000004e-06, 'tol': 0.0001}\n",
            "  0%|          | 0/1000 [00:00<?, ?it/s, best loss: ?]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-c16eebf3f7a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtpe_algo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mtrials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtpe_trials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             max_evals=1000)\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;31m# Print best parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspace_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         )\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar)\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             show_progressbar=show_progressbar)\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    405\u001b[0m                     show_progressbar=show_progressbar)\n\u001b[1;32m    406\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                         \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 844\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-c16eebf3f7a0>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrn_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNbSvmClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tol'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdual\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m       \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrn_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrn_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m       \u001b[0;31m#print(str(train_features[val_idx].shape)+\" and \"+str(training_labels[val_idx].shape))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0mclass_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-c4112fa7dc5c>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mx_nb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdual\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_nb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1599\u001b[0m                       \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m                       sample_weight=sample_weight)\n\u001b[0;32m-> 1601\u001b[0;31m             for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n\u001b[0m\u001b[1;32m   1602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mfold_coefs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfold_coefs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    560\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}